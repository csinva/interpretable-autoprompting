{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file, we first load the fMRI data for each voxel we want to prediction.\n",
    "- We then calculate a text description for each voxel, which we hope will corresponding to a semantic concept\n",
    "- Next, we visualize these concepts and select a couple best ones\n",
    "- We visualize the best ones\n",
    "- We then validate that the best ones are spatially close together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "import os\n",
    "import cortex # brain viz library\n",
    "from data_utils import neuro\n",
    "from transformers import AutoModel, AutoModelForCausalLM, AutoTokenizer\n",
    "from model_utils import suffix\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lists = neuro.fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 15)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_words = 15\n",
    "word_lists = word_lists[:, :n_words]\n",
    "word_lists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'EleutherAI/gpt-j-6B' # 'gpt2-medium'\n",
    "device = 'cuda'\n",
    "save_dir = f'/home/chansingh/mntv1/fmri/logits_{checkpoint}'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    checkpoint, output_hidden_states=False).to(device)\n",
    "batch_size = 10 # make sure this divides into word_lists.shape[0]\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt_from_word_list(word_list):\n",
    "    s = 'The following list of words are all part of the same semantic category: '\n",
    "    s += ', '.join(word_list)\n",
    "    s += '.\\nThe semantic category they all belong to, in one word, is'\n",
    "    return s\n",
    "\n",
    "# run one example\n",
    "s = make_prompt_from_word_list(word_lists[0]) \n",
    "ex_inputs = tokenizer([s], padding='longest', return_tensors='pt')\n",
    "next_token_logits = suffix.get_next_token_logits(ex_inputs, model).squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 186/1000 [01:33<07:21,  1.85it/s]"
     ]
    }
   ],
   "source": [
    "# iterate and store over all examples\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "vocab_size = next_token_logits.shape[-1]\n",
    "all_logits = np.zeros((word_lists.shape[0], vocab_size))\n",
    "\n",
    "for i in tqdm(range(0, word_lists.shape[0], batch_size)): # batch_size is step\n",
    "    s = [make_prompt_from_word_list(wlist) for wlist in word_lists[i: i + batch_size]]\n",
    "    ex_inputs = tokenizer(s, padding='longest', return_tensors='pt').to(device)\n",
    "    next_token_logits = suffix.get_next_token_logits(ex_inputs, model).detach().cpu().numpy()\n",
    "    all_logits[i: i + batch_size] = next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(all_logits, open(oj(save_dir, 'all_logits.pkl'), 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode top tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chansingh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' material' ' metal' ' shape' ' surface' ' simply' ' fabric' ' materials'\n",
      " ' packaging' ' box' ' something' ' CON' ' plastic' ' card' ' colour'\n",
      " ' container' ' table' ' wood' ' part' ' cloth' ' Material']\n"
     ]
    }
   ],
   "source": [
    "next_token_logits = all_logits[0]\n",
    "\n",
    "# decode top tokens\n",
    "top_k_inds = np.arange(next_token_logits.size)\n",
    "top_k_inds = top_k_inds[np.argsort(next_token_logits[top_k_inds])][::-1]\n",
    "top_decoded_tokens = np.array(\n",
    "    [tokenizer.decode(ind) for ind in top_k_inds])\n",
    "\n",
    "# remove nonsense\n",
    "STOPWORDS = suffix.get_stopwords()\n",
    "PHRASING_STOPWORDS = ['called']\n",
    "disallowed_idxs = np.array([\n",
    "    # general\n",
    "    s.isspace() # space\n",
    "    or all(c in string.punctuation for c in s.strip()) # punc\n",
    "    or len(s) <= 2\n",
    "\n",
    "    # keywords\n",
    "    or s.lower().strip() in STOPWORDS # stopwords \n",
    "    or s.lower().strip() in PHRASING_STOPWORDS # stopwords  \n",
    "\n",
    "    # check if it is one of the inputs\n",
    "    or s.strip().lower() in word_lists[0] # not one of the inputs\n",
    "    or s.strip().lower() + 's' in word_lists[0] # plural not in inputs\n",
    "    or s.strip().lower() in [word + 's' for word in word_lists[0]] # singular not in inputs\n",
    "    for s in top_decoded_tokens],\n",
    "    dtype=bool)\n",
    "top_k_inds = top_k_inds[~disallowed_idxs]\n",
    "top_decoded_tokens = top_decoded_tokens[~disallowed_idxs]\n",
    "print(top_decoded_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.autoprompt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14b67e045ab4e623bbd9f77d231431043e985fd8f169f266aea842e78b0c1086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
