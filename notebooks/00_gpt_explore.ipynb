{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "checkpoint = \"EleutherAI/gpt-neo-125M\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some top models and their sizes.\n",
    "\n",
    "**open-source**\n",
    "- GPT-Neo 125 mil\n",
    "- GPT-Neo 1.3 bil (same as GPT-3 Babbage)\n",
    "- GPT-2 1.5 bil\n",
    "- GPT-Neo 2.7 bil\n",
    "- GPT-J 6 bil\n",
    "- GPT-NeoX 20 bil\n",
    "- Bloom: ranges from 350m to 176 bil\n",
    "\n",
    "**closed-source**\n",
    "- GPT-3: 175 bil at biggest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# high-level api we want\n",
    "https://huggingface.co/docs/transformers/internal/generation_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010823488235473633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.json",
       "rate": null,
       "total": 1042301,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514132bc6df24387b0b96d2fc6c5dd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011362075805664062,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading merges.txt",
       "rate": null,
       "total": 456318,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca688aa773924f13bea1e4641f9b1fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011530876159667969,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 665,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a3eb24de134dc48acd1657766a53cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011459827423095703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 548118077,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7234b9cf90584701b1b99625fe067991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/chansingh/.local/lib/python3.8/site-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute and \", return_tensors=\"pt\")\n",
    "generation_output = model.generate(**inputs,\n",
    "                                   return_dict_in_generate=True,\n",
    "                                   output_scores=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try to get gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[1026,  373,  922]]), 'attention_mask': tensor([[1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"It was good\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, return_tensors=\"pt\")\n",
    "# inputs['input_ids'] = inputs['input_ids'].float()\n",
    "# inputs['input_ids'].requires_grad = True\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, output_hidden_states=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# loss = outputs['logits'].sum()\n",
    "# outputs['logits'].retain_grad()\n",
    "# loss.backward(retain_graph=True)\n",
    "# outputs['logits'].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 50257])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(55.4541, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['hidden_states'][0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('transformer',\n",
       "               GPTNeoModel(\n",
       "                 (wte): Embedding(50257, 768)\n",
       "                 (wpe): Embedding(2048, 768)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "                 (h): ModuleList(\n",
       "                   (0): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (1): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (2): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (3): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (4): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (5): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (6): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (7): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (8): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (9): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (10): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (11): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "               )),\n",
       "              ('lm_head',\n",
       "               Linear(in_features=768, out_features=50257, bias=False))]),\n",
       " 'config': GPTNeoConfig {\n",
       "   \"_name_or_path\": \"EleutherAI/gpt-neo-125M\",\n",
       "   \"activation_function\": \"gelu_new\",\n",
       "   \"architectures\": [\n",
       "     \"GPTNeoForCausalLM\"\n",
       "   ],\n",
       "   \"attention_dropout\": 0,\n",
       "   \"attention_layers\": [\n",
       "     \"global\",\n",
       "     \"local\",\n",
       "     \"global\",\n",
       "     \"local\",\n",
       "     \"global\",\n",
       "     \"local\",\n",
       "     \"global\",\n",
       "     \"local\",\n",
       "     \"global\",\n",
       "     \"local\",\n",
       "     \"global\",\n",
       "     \"local\"\n",
       "   ],\n",
       "   \"attention_types\": [\n",
       "     [\n",
       "       [\n",
       "         \"global\",\n",
       "         \"local\"\n",
       "       ],\n",
       "       6\n",
       "     ]\n",
       "   ],\n",
       "   \"bos_token_id\": 50256,\n",
       "   \"embed_dropout\": 0,\n",
       "   \"eos_token_id\": 50256,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"hidden_size\": 768,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": null,\n",
       "   \"layer_norm_epsilon\": 1e-05,\n",
       "   \"max_position_embeddings\": 2048,\n",
       "   \"model_type\": \"gpt_neo\",\n",
       "   \"num_heads\": 12,\n",
       "   \"num_layers\": 12,\n",
       "   \"output_hidden_states\": true,\n",
       "   \"resid_dropout\": 0,\n",
       "   \"summary_activation\": null,\n",
       "   \"summary_first_dropout\": 0.1,\n",
       "   \"summary_proj_to_labels\": true,\n",
       "   \"summary_type\": \"cls_index\",\n",
       "   \"summary_use_proj\": true,\n",
       "   \"transformers_version\": \"4.21.2\",\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 50257,\n",
       "   \"window_size\": 256\n",
       " },\n",
       " 'name_or_path': 'EleutherAI/gpt-neo-125M',\n",
       " 'warnings_issued': {}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for k in model._modules:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=50257, bias=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules['lm_head']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state\n",
      "past_key_values\n",
      "hidden_states\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(55.4541, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = model._modules['transformer']\n",
    "out = trans(inputs['input_ids'])\n",
    "for k in out:\n",
    "    print(k)\n",
    "h = out['hidden_states']\n",
    "h[0].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
