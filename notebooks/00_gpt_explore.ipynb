{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from copy import deepcopy\n",
    "checkpoint = \"EleutherAI/gpt-neo-2.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some top models and their sizes.\n",
    "\n",
    "**open-source**\n",
    "- GPT-Neo 125 mil\n",
    "- GPT-Neo 1.3 bil (same as GPT-3 Babbage)\n",
    "- GPT-2 1.5 bil\n",
    "- GPT-Neo 2.7 bil\n",
    "- GPT-J 6 bil\n",
    "- GPT-NeoX 20 bil\n",
    "- Bloom: ranges from 350m to 176 bil\n",
    "\n",
    "**closed-source**\n",
    "- GPT-3: 175 bil at biggest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to get gradients\n",
    "Link to understand GPT models better: [minGPT](https://github.com/karpathy/minGPT/blob/master/mingpt/model.py). Word embeddings are summed with positional embeddings then passed on.\n",
    "\n",
    "Architecture for [GPT-Neo](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neo/modeling_gpt_neo.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[16, 10, 18, 28, 19]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "last_hidden_state\n",
      "past_key_values\n",
      "hidden_states\n"
     ]
    }
   ],
   "source": [
    "# prepare inputs\n",
    "raw_inputs = [\n",
    "    \"1+3=4\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, return_tensors=\"pt\")\n",
    "# inputs['input_ids'] = inputs['input_ids'].float()\n",
    "# inputs['input_ids'].requires_grad = True\n",
    "print(inputs)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, output_hidden_states=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# loss = outputs['logits'].sum()\n",
    "# outputs['logits'].retain_grad()\n",
    "# loss.backward(retain_graph=True)\n",
    "# outputs['logits'].grad\n",
    "\n",
    "# go through the model\n",
    "trans = model._modules['transformer']\n",
    "lm_head = model._modules['lm_head']\n",
    "out = trans(inputs['input_ids'])\n",
    "for k in out:\n",
    "    print(k)\n",
    "h = out['hidden_states'] # tuple of (layer x (batch_size, seq_len, hidden_size))\n",
    "logits = lm_head(h[-1])  # select logits using last layer\n",
    "\n",
    "# we got the same logits by going through the model\n",
    "assert logits.shape == outputs['logits'].shape # tensor (batch_size, seq_len, vocab_size)\n",
    "assert logits.sum() == outputs['logits'].sum()\n",
    "assert logits.max() == outputs['logits'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: 1+3=4\n",
      "decoded text from hidden states .\\\\4$\n",
      "0: ___1___ --> ___.___\n",
      "1: ___+___ --> ___\\___\n",
      "2: ___3___ --> ___\\___\n",
      "3: ___=___ --> ___4___\n",
      "4: ___4___ --> ___$___\n"
     ]
    }
   ],
   "source": [
    "# naive check\n",
    "print('input text:', tokenizer.decode(inputs['input_ids'][0]))\n",
    "\n",
    "# top word embeddings\n",
    "decoded_toks = tokenizer.decode(logits[0].argmax(axis=-1))\n",
    "print('decoded text from hidden states', decoded_toks)\n",
    "\n",
    "# dissect token-by-token\n",
    "for i, tok in enumerate(inputs.tokens()):\n",
    "    print(f'{i}: ___{tok}___ --> ___{decoded_toks[i]}___')\n",
    "# top_word_embeddings = logits.argmax(axis=2)\n",
    "# print(top_word_embeddings.shape, top_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alter the model\n",
    "# trans.wte.forward(inputs['input_ids'])\n",
    "w_embed = trans.wte.weight # vocab_size, embed_dim\n",
    "vocab_size = w_embed.shape[0]\n",
    "embed_size = w_embed.shape[1]\n",
    "\n",
    "\"\"\"\n",
    "emb_linear = nn.Linear(in_features=vocab_size, out_features=embed_size, bias=False)\n",
    "print(emb_linear.weight.shape, w_embed.shape)\n",
    "emb_linear.weight = nn.Parameter(w_embed.T)\n",
    "\"\"\"\n",
    "\n",
    "unemb_linear = nn.Linear(in_features=embed_size, out_features=vocab_size, bias=False)\n",
    "pinv = torch.linalg.pinv(w_embed)\n",
    "unemb_linear.weight = nn.Parameter(pinv.T)\n",
    "\n",
    "# make sure unembedding works\n",
    "ids = torch.Tensor([[16, 2, 3]]).int()\n",
    "embs = trans.wte.forward(ids)\n",
    "\n",
    "unembedded_onehot = unemb_linear(embs)\n",
    "unembedded_ids = unembedded_onehot.argmax(axis=-1)\n",
    "assert torch.all(unembedded_ids == ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do forward pass with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = trans.wte.forward(ids)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, output_hidden_states=True)\n",
    "outputs_using_embeds = model(inputs_embeds=embeds)\n",
    "outputs = model(input_ids=ids)\n",
    "\n",
    "assert outputs['logits'].sum() == outputs_using_embeds['logits'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = trans.wte.forward(ids)\n",
    "embeds.retain_grad()\n",
    "outputs = model(inputs_embeds=embeds)\n",
    "loss = outputs['logits'].sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  9575.5391,  -1355.6121,  -2623.6182,  ...,  12424.7637,\n",
       "           -5506.7495,  -9292.2803],\n",
       "         [ 12695.8623,  10005.6162,  17801.9453,  ...,   7830.0645,\n",
       "           24521.3984,   4288.2812],\n",
       "         [ -9870.7490,   -223.3108,  -3130.4512,  ...,  15149.9629,\n",
       "          -23681.2461,  -5486.9058]]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeds.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API ref\n",
    "https://huggingface.co/docs/transformers/internal/generation_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010823488235473633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.json",
       "rate": null,
       "total": 1042301,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514132bc6df24387b0b96d2fc6c5dd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011362075805664062,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading merges.txt",
       "rate": null,
       "total": 456318,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca688aa773924f13bea1e4641f9b1fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011530876159667969,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 665,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a3eb24de134dc48acd1657766a53cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011459827423095703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 548118077,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7234b9cf90584701b1b99625fe067991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/chansingh/.local/lib/python3.8/site-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute and \", return_tensors=\"pt\")\n",
    "generation_output = model.generate(**inputs,\n",
    "                                   return_dict_in_generate=True,\n",
    "                                   output_scores=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
