{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "checkpoint = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some top models and their sizes.\n",
    "\n",
    "**open-source**\n",
    "- GPT-Neo 125 mil\n",
    "- GPT-Neo 1.3 bil (same as GPT-3 Babbage)\n",
    "- GPT-2 1.5 bil\n",
    "- GPT-Neo 2.7 bil\n",
    "- GPT-J 6 bil\n",
    "- GPT-NeoX 20 bil\n",
    "- Bloom: ranges from 350m to 176 bil\n",
    "\n",
    "**closed-source**\n",
    "- GPT-3: 175 bil at biggest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to get gradients\n",
    "Link to understand GPT models better: [minGPT](https://github.com/karpathy/minGPT/blob/master/mingpt/model.py). Word embeddings are summed with positional embeddings then passed on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[14662,   318,   644,  4325,   618,   345,   821,  8179]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "last_hidden_state\n",
      "past_key_values\n",
      "hidden_states\n"
     ]
    }
   ],
   "source": [
    "# prepare inputs\n",
    "raw_inputs = [\n",
    "    \"Life is what happens when you're busy\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, return_tensors=\"pt\")\n",
    "# inputs['input_ids'] = inputs['input_ids'].float()\n",
    "# inputs['input_ids'].requires_grad = True\n",
    "print(inputs)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, output_hidden_states=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# loss = outputs['logits'].sum()\n",
    "# outputs['logits'].retain_grad()\n",
    "# loss.backward(retain_graph=True)\n",
    "# outputs['logits'].grad\n",
    "\n",
    "# go through the model\n",
    "trans = model._modules['transformer']\n",
    "lm_head = model._modules['lm_head']\n",
    "out = trans(inputs['input_ids'])\n",
    "for k in out:\n",
    "    print(k)\n",
    "h = out['hidden_states'] # tuple of (layer x (batch_size, seq_len, hidden_size))\n",
    "logits = lm_head(h[-1])  # select logits using last layer\n",
    "\n",
    "# we got the same logits by going through the model\n",
    "assert logits.shape == outputs['logits'].shape # tensor (batch_size, seq_len, vocab_size)\n",
    "assert logits.sum() == outputs['logits'].sum()\n",
    "assert logits.max() == outputs['logits'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_embeddings = logits.argmax(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 11]) tensor([[286, 257, 356, 618, 345, 423, 407,  13, 257, 661,  13]])\n"
     ]
    }
   ],
   "source": [
    "print(top_word_embeddings.shape, top_word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257])\n",
      "0  of\n",
      "torch.Size([50257])\n",
      "1  a\n",
      "torch.Size([50257])\n",
      "2  we\n",
      "torch.Size([50257])\n",
      "3  when\n",
      "torch.Size([50257])\n",
      "4  you\n",
      "torch.Size([50257])\n",
      "5  have\n",
      "torch.Size([50257])\n",
      "6  not\n",
      "torch.Size([50257])\n",
      "7 .\n",
      "torch.Size([50257])\n",
      "8  a\n",
      "torch.Size([50257])\n",
      "9  people\n",
      "torch.Size([50257])\n",
      "10 .\n"
     ]
    }
   ],
   "source": [
    "for seq_pos in range(logits.shape[1]):\n",
    "    logits_seq_pos = logits[0, seq_pos] \n",
    "    top_word_idx = logits_seq_pos.argmax()\n",
    "    top_word_idxs = logits_seq_pos[logits_seq_pos[np.argsort(a[ind])]}\n",
    "    print(logits_seq_pos.shape, top_word_idx)\n",
    "    print(seq_pos, tokenizer.decode(top_word_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer([\n",
    "    \"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do',\n",
       " 'Ġnot',\n",
       " 'Ġmedd',\n",
       " 'le',\n",
       " 'Ġin',\n",
       " 'Ġthe',\n",
       " 'Ġaffairs',\n",
       " 'Ġof',\n",
       " 'Ġwizards',\n",
       " ',',\n",
       " 'Ġfor',\n",
       " 'Ġthey',\n",
       " 'Ġare',\n",
       " 'Ġsubtle',\n",
       " 'Ġand',\n",
       " 'Ġquick',\n",
       " 'Ġto',\n",
       " 'Ġanger',\n",
       " '.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode(encoded_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do not meddle in the affairs of wizards, for they are subtle and quick to anger.'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2390532.5000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['logits'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['hidden_states'][0].shape # tuple of (layer x (batch_size, seq_len, hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs['hidden_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('transformer',\n",
       "               GPTNeoModel(\n",
       "                 (wte): Embedding(50257, 768)\n",
       "                 (wpe): Embedding(2048, 768)\n",
       "                 (drop): Dropout(p=0.0, inplace=False)\n",
       "                 (h): ModuleList(\n",
       "                   (0): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (1): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (2): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (3): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (4): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (5): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (6): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (7): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (8): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (9): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (10): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                   (11): GPTNeoBlock(\n",
       "                     (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (attn): GPTNeoAttention(\n",
       "                       (attention): GPTNeoSelfAttention(\n",
       "                         (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "                         (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "                         (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                       )\n",
       "                     )\n",
       "                     (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                     (mlp): GPTNeoMLP(\n",
       "                       (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (act): NewGELUActivation()\n",
       "                       (dropout): Dropout(p=0.0, inplace=False)\n",
       "                     )\n",
       "                   )\n",
       "                 )\n",
       "                 (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "               )),\n",
       "              ('lm_head',\n",
       "               Linear(in_features=768, out_features=50257, bias=False))]),\n",
       " 'config': GPTNeoConfig {\n",
       "   \"_name_or_path\": \"EleutherAI/gpt-neo-125M\",\n",
       "   \"activation_function\": \"gelu_new\",\n",
       "   \"architectures\": [\n",
       "     \"GPTNeoForCausalLM\"\n",
       "   ],\n",
       "   \"attention_dropout\": 0,\n",
       "   \"attention_layers\": [\n",
       "     \"global\",\n",
       "     \"local\",\n",
       "     \"global\",\n",
       "     \"local\",\n",
       "     \"global\",\n",
       "     \"local\",\n",
       "     \"global\",\n",
       "     \"local\",\n",
       "     \"global\",\n",
       "     \"local\",\n",
       "     \"global\",\n",
       "     \"local\"\n",
       "   ],\n",
       "   \"attention_types\": [\n",
       "     [\n",
       "       [\n",
       "         \"global\",\n",
       "         \"local\"\n",
       "       ],\n",
       "       6\n",
       "     ]\n",
       "   ],\n",
       "   \"bos_token_id\": 50256,\n",
       "   \"embed_dropout\": 0,\n",
       "   \"eos_token_id\": 50256,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"hidden_size\": 768,\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"intermediate_size\": null,\n",
       "   \"layer_norm_epsilon\": 1e-05,\n",
       "   \"max_position_embeddings\": 2048,\n",
       "   \"model_type\": \"gpt_neo\",\n",
       "   \"num_heads\": 12,\n",
       "   \"num_layers\": 12,\n",
       "   \"output_hidden_states\": true,\n",
       "   \"resid_dropout\": 0,\n",
       "   \"summary_activation\": null,\n",
       "   \"summary_first_dropout\": 0.1,\n",
       "   \"summary_proj_to_labels\": true,\n",
       "   \"summary_type\": \"cls_index\",\n",
       "   \"summary_use_proj\": true,\n",
       "   \"transformers_version\": \"4.21.2\",\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 50257,\n",
       "   \"window_size\": 256\n",
       " },\n",
       " 'name_or_path': 'EleutherAI/gpt-neo-125M',\n",
       " 'warnings_issued': {}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for k in model._modules:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_hidden_state\n",
      "past_key_values\n",
      "hidden_states\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-2390532.5000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API ref\n",
    "https://huggingface.co/docs/transformers/internal/generation_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010823488235473633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.json",
       "rate": null,
       "total": 1042301,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514132bc6df24387b0b96d2fc6c5dd0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011362075805664062,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading merges.txt",
       "rate": null,
       "total": 456318,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca688aa773924f13bea1e4641f9b1fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011530876159667969,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 665,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a3eb24de134dc48acd1657766a53cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.011459827423095703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 548118077,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7234b9cf90584701b1b99625fe067991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/chansingh/.local/lib/python3.8/site-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute and \", return_tensors=\"pt\")\n",
    "generation_output = model.generate(**inputs,\n",
    "                                   return_dict_in_generate=True,\n",
    "                                   output_scores=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
