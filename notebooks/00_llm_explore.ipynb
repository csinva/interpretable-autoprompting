{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from copy import deepcopy\n",
    "checkpoint = \"EleutherAI/gpt-neo-2.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some top models and their sizes.\n",
    "\n",
    "**open-source**\n",
    "- GPT-Neo 125 mil\n",
    "- GPT-Neo 1.3 bil (same as GPT-3 Babbage)\n",
    "- GPT-2 1.5 bil\n",
    "- GPT-Neo 2.7 bil\n",
    "- GPT-J 6 bil\n",
    "- GPT-NeoX 20 bil\n",
    "- Bloom: ranges from 350m to 176 bil\n",
    "\n",
    "**closed-source**\n",
    "- GPT-3: 175 bil at biggest\n",
    "\n",
    "Link to understand GPT models better: [minGPT](https://github.com/karpathy/minGPT/blob/master/mingpt/model.py). Word embeddings are summed with positional embeddings then passed on.\n",
    "\n",
    "Architecture for [GPT-Neo](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neo/modeling_gpt_neo.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test: transformer + lm_head = original model + probabilities for next tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text: 1+3=4\n",
      "decoded text from hidden states .\\\\4$\n",
      "top predicted tokens at this position:\n",
      "0: ___1___ --> ___.___\n",
      "1: ___+___ --> ___\\___\n",
      "2: ___3___ --> ___\\___\n",
      "3: ___=___ --> ___4___\n",
      "4: ___4___ --> ___$___\n"
     ]
    }
   ],
   "source": [
    "# prepare inputs\n",
    "raw_inputs = [\"1+3=4\"]\n",
    "inputs = tokenizer(raw_inputs, return_tensors=\"pt\")\n",
    "\n",
    "# predict\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# show that the lm_head is producing logits via linear transformation\n",
    "trans = model._modules['transformer']\n",
    "lm_head = model._modules['lm_head']\n",
    "out = trans(inputs['input_ids'])\n",
    "h = out['hidden_states'] # tuple of (layer x (batch_size, seq_len, hidden_size))\n",
    "logits = lm_head(h[-1])  # select logits using last layer\n",
    "\n",
    "# we got the same logits by going through the model\n",
    "assert logits.shape == outputs['logits'].shape # tensor (batch_size, seq_len, vocab_size)\n",
    "assert logits.sum() == outputs['logits'].sum()\n",
    "assert logits.max() == outputs['logits'].max()\n",
    "\n",
    "# naive check\n",
    "print('input text:', tokenizer.decode(inputs['input_ids'][0]))\n",
    "\n",
    "# top word embeddings\n",
    "decoded_toks = tokenizer.decode(logits[0].argmax(axis=-1))\n",
    "print('decoded text from hidden states', decoded_toks)\n",
    "\n",
    "# dissect token-by-token\n",
    "print('top predicted tokens at this position:')\n",
    "for i, tok in enumerate(inputs.tokens()):\n",
    "    print(f'{i}: ___{tok}___ --> ___{decoded_toks[i]}___')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set up projecting back to word-vectors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embedding matrix\n",
    "w_embed = trans.wte.weight # vocab_size, embed_dim\n",
    "vocab_size = w_embed.shape[0]\n",
    "embed_size = w_embed.shape[1]\n",
    "\n",
    "# invert for unembedding\n",
    "unemb_linear = nn.Linear(in_features=embed_size, out_features=vocab_size, bias=False)\n",
    "pinv = torch.linalg.pinv(w_embed)\n",
    "unemb_linear.weight = nn.Parameter(pinv.T)\n",
    "\n",
    "# make sure unembedding works\n",
    "ids = torch.Tensor([[16, 2, 3]]).int()\n",
    "embs = trans.wte.forward(ids)\n",
    "unembedded_onehot = unemb_linear(embs)\n",
    "unembedded_ids = unembedded_onehot.argmax(axis=-1)\n",
    "assert torch.all(unembedded_ids == ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get gradient wrt embedding vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  9575.5391,  -1355.6121,  -2623.6182,  ...,  12424.7637,\n",
       "           -5506.7495,  -9292.2803],\n",
       "         [ 12695.8623,  10005.6162,  17801.9453,  ...,   7830.0645,\n",
       "           24521.3984,   4288.2812],\n",
       "         [ -9870.7490,   -223.3108,  -3130.4512,  ...,  15149.9629,\n",
       "          -23681.2461,  -5486.9058]]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that original outputs match outputs when using embedding\n",
    "embeds = trans.wte.forward(ids)\n",
    "outputs_using_embeds = model(inputs_embeds=embeds)\n",
    "outputs = model(input_ids=ids)\n",
    "assert outputs['logits'].sum() == outputs_using_embeds['logits'].sum()\n",
    "\n",
    "# get gradient\n",
    "embeds = trans.wte.forward(ids)\n",
    "embeds.retain_grad()\n",
    "outputs = model(inputs_embeds=embeds)\n",
    "loss = outputs['logits'].sum()\n",
    "loss.backward()\n",
    "embeds.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
