{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1813f150-d26e-4d76-9d74-d54b56b7f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from os.path import join as oj\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import iprompt.data\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "from iprompt.data import TASKS_GALACTICA\n",
    "from imodelsx import explain_dataset_iprompt, get_add_two_numbers_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "280bbfbc",
   "metadata": {},
   "source": [
    "# BBBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "631d571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = TASKS_GALACTICA['bbbp']\n",
    "df = task['gen_func']()\n",
    "input_strings = df['input'].values\n",
    "output_strings = df['output'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ef8eb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 44.56 GiB total capacity; 42.45 GiB already allocated; 134.81 MiB free; 43.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# get a simple dataset of adding two numbers\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# explain the relationship between the inputs and outputs\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# with a natural-language prompt string\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m prompts, metadata \u001b[39m=\u001b[39m explain_dataset_iprompt(\n\u001b[1;32m      6\u001b[0m     input_strings\u001b[39m=\u001b[39;49minput_strings,\n\u001b[1;32m      7\u001b[0m     output_strings\u001b[39m=\u001b[39;49moutput_strings,\n\u001b[1;32m      8\u001b[0m     \u001b[39m# checkpoint='EleutherAI/gpt-j-6B', # which language model to use\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m     checkpoint\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfacebook/galactica-6.7b\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m# which language model to use\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m     preprefix\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mAnswer yes if the compound\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     11\u001b[0m     num_learned_tokens\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \u001b[39m# how long of a prompt to learn\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m     n_shots\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, \u001b[39m# shots per example\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m     n_epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m, \u001b[39m# how many epochs to search\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \u001b[39m# how much to print\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m     llm_float16\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m# whether to load the model in float_16\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/imodelsX/imodelsx/iprompt/api.py:458\u001b[0m, in \u001b[0;36mexplain_dataset_iprompt\u001b[0;34m(input_strings, output_strings, checkpoint, num_learned_tokens, save_dir, lr, pop_size, num_mutations, num_random_generations, generation_repetition_penalty, early_stopping_steps, llm_float16, gamma, batch_size, max_length, n_epochs, n_shots, preprefix, single_shot_loss, accum_grad_over_epoch, max_n_datapoints, max_n_steps, epoch_save_interval, mask_possible_answers, model_cls, verbose, seed)\u001b[0m\n\u001b[1;32m    456\u001b[0m random\u001b[39m.\u001b[39mseed(seed)\n\u001b[1;32m    457\u001b[0m r \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[0;32m--> 458\u001b[0m r \u001b[39m=\u001b[39m train_model(\n\u001b[1;32m    459\u001b[0m     r\u001b[39m=\u001b[39;49mr,\n\u001b[1;32m    460\u001b[0m     input_strs\u001b[39m=\u001b[39;49minput_strings,\n\u001b[1;32m    461\u001b[0m     output_strs\u001b[39m=\u001b[39;49moutput_strings,\n\u001b[1;32m    462\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m    463\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m    464\u001b[0m     save_dir\u001b[39m=\u001b[39;49msave_dir,\n\u001b[1;32m    465\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    466\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    467\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    468\u001b[0m     mask_possible_answers\u001b[39m=\u001b[39;49mmask_possible_answers,\n\u001b[1;32m    469\u001b[0m     n_epochs\u001b[39m=\u001b[39;49mn_epochs,\n\u001b[1;32m    470\u001b[0m     n_shots\u001b[39m=\u001b[39;49mn_shots,\n\u001b[1;32m    471\u001b[0m     single_shot_loss\u001b[39m=\u001b[39;49msingle_shot_loss,\n\u001b[1;32m    472\u001b[0m     accum_grad_over_epoch\u001b[39m=\u001b[39;49maccum_grad_over_epoch,\n\u001b[1;32m    473\u001b[0m     max_n_datapoints\u001b[39m=\u001b[39;49mmax_n_datapoints,\n\u001b[1;32m    474\u001b[0m     max_n_steps\u001b[39m=\u001b[39;49mmax_n_steps,\n\u001b[1;32m    475\u001b[0m     epoch_save_interval\u001b[39m=\u001b[39;49mepoch_save_interval,\n\u001b[1;32m    476\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    477\u001b[0m )\n\u001b[1;32m    478\u001b[0m \u001b[39mreturn\u001b[39;00m r[\u001b[39m'\u001b[39m\u001b[39mprefixes\u001b[39m\u001b[39m'\u001b[39m], r\n",
      "File \u001b[0;32m~/imodelsX/imodelsx/iprompt/api.py:114\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(r, input_strs, output_strs, model, tokenizer, save_dir, lr, batch_size, max_length, n_epochs, n_shots, single_shot_loss, accum_grad_over_epoch, max_n_datapoints, max_n_steps, epoch_save_interval, mask_possible_answers, verbose)\u001b[0m\n\u001b[1;32m    111\u001b[0m     dset \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_pandas(df)\n\u001b[1;32m    112\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mloading model...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m    115\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m    116\u001b[0m     dset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, drop_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    118\u001b[0m \u001b[39m# optimizer\u001b[39;00m\n",
      "File \u001b[0;32m~/.autoprompt/lib/python3.8/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/.autoprompt/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.autoprompt/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 579 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/.autoprompt/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    581\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.autoprompt/lib/python3.8/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.autoprompt/lib/python3.8/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 44.56 GiB total capacity; 42.45 GiB already allocated; 134.81 MiB free; 43.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# get a simple dataset of adding two numbers\n",
    "\n",
    "# explain the relationship between the inputs and outputs\n",
    "# with a natural-language prompt string\n",
    "prompts, metadata = explain_dataset_iprompt(\n",
    "    input_strings=input_strings,\n",
    "    output_strings=output_strings,\n",
    "    # checkpoint='EleutherAI/gpt-j-6B', # which language model to use\n",
    "    checkpoint=\"facebook/galactica-6.7b\", # which language model to use\n",
    "    preprefix='Answer yes if the compound',\n",
    "    num_learned_tokens=10, # how long of a prompt to learn\n",
    "    n_shots=5, # shots per example\n",
    "    n_epochs=15, # how many epochs to search\n",
    "    verbose=0, # how much to print\n",
    "    llm_float16=False, # whether to load the model in float_16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa0170d",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be248f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/galactica-6.7b\")\n",
    "model = OPTForCausalLM.from_pretrained(\"facebook/galactica-6.7b\", device_map=\"auto\", torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8719599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(model, input_text):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids, max_length=200)\n",
    "    return tokenizer.decode(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fb4666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[START_I_SMILES]CC1=CC=C(C=C1)C2=NN=C(N2C3=CC=CC=C3)SCC(=O)NC4=CC=CC=C4OC[END_I_SMILES]\\n\\n### Molecular Formula\\n\\nC24H22N4O2S\\n\\n## Chemical and Physical Properties\\n\\nThe following are chemical properties for 2-[[4,5-bis(p-tolyl)-1,2,4-triazol-3-yl]sulfanyl]-N-(2-methoxyphenyl)acetamide.\\n\\n### Computed Properties\\n\\n| Property Name | Property Value\\n| --- | ----------- |\\n| Molecular Weight | 430.5\\n| XLogP3-AA Log P | 4.9\\n| Hydrogen Bond Donor Count | 1\\n| Hydrogen Bond Acceptor Count |'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(model, \"[START_I_SMILES]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".autoprompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "14b67e045ab4e623bbd9f77d231431043e985fd8f169f266aea842e78b0c1086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
