{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to assess the generalization accuracy of a generated suffix, assuming a data-split was used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from types import SimpleNamespace\n",
    "from datasets import Dataset\n",
    "from os.path import join as oj\n",
    "import pickle as pkl\n",
    "import os\n",
    "import dvu\n",
    "dvu.set_style()\n",
    "import analyze_utils\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import data\n",
    "from model_utils import prompt_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load results and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "file must have 'read' and 'readline' attributes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m prompts_df \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../results/autoprompt_sentiment/prompts.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: file must have 'read' and 'readline' attributes"
     ]
    }
   ],
   "source": [
    "prompts_df = pkl.load('../results/autoprompt_sentiment/prompts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fake_args:\n",
    "    template_num_task_phrasing = 0\n",
    "    max_dset_size = 1000\n",
    "    max_digit = 10\n",
    "    seed = 1\n",
    "    train_split_frac = 0.75\n",
    "\n",
    "    # these will be varied\n",
    "    n_shots = 1\n",
    "    task_name = 'add_two'\n",
    "\n",
    "\n",
    "args = fake_args()\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "ds = []\n",
    "results_dir = '../results/generalization_acc/'\n",
    "# checkpoints = []\n",
    "checkpoints = ['gpt2-medium', 'EleutherAI/gpt-j-6B', 'gpt2-xl']\n",
    "for n_shots in [1, 5, 10]:\n",
    "    for checkpoint in checkpoints:  # , 'gpt2-xl']:['gpt2-medium']:\n",
    "        save_name = f'baseline_accs_{checkpoint.replace(\"/\", \"___\")}___nshots={n_shots}.pkl'\n",
    "        d = pd.DataFrame.from_dict(pkl.load(open(\n",
    "            oj(results_dir, save_name, 'rb'))))\n",
    "        ds.append(deepcopy(d))\n",
    "df = pd.concat(ds)\n",
    "df['prompt'][df['prompt'] == ''] = 'no prompt'\n",
    "df = df.sort_values(by=['task_name', 'prompt'])\n",
    "\n",
    "for checkpoint in checkpoints:\n",
    "    d = df[df.checkpoint == checkpoint]\n",
    "    d = d[d.n_shots == 5]\n",
    "    plt.figure(figsize=(6, 8))\n",
    "    ax = sns.barplot(y='task_name', x='acc', data=d,\n",
    "                     hue=(d['prompt'] + ', n_ex=' + d['n_shots'].astype(str)))\n",
    "    ax.grid()\n",
    "    plt.xlim(0, 100)\n",
    "    plt.title(checkpoint)\n",
    "    plt.savefig(\n",
    "        oj(results_dir, f'baseline_accs_{checkpoint.replace(\"/\", \"___\")}.pdf'), bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading EleutherAI/gpt-j-6B\n"
     ]
    }
   ],
   "source": [
    "class fake_args:\n",
    "    template_num_task_phrasing = 0\n",
    "    max_dset_size = 1000\n",
    "    max_digit = 10\n",
    "    seed = 1\n",
    "    train_split_frac = 0.75\n",
    "\n",
    "    # these will be varied\n",
    "    n_shots = 1\n",
    "    task_name = 'add_two'\n",
    "args = fake_args()\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "task_names = ['task1191_food_veg_nonveg']\n",
    "batch_sizes = {\n",
    "    'gpt2-medium': 32,\n",
    "    'EleutherAI/gpt-j-6B': 8,\n",
    "    'EleutherAI/gpt-neox-20b': 1,\n",
    "}\n",
    "checkpoint = 'EleutherAI/gpt-j-6B'\n",
    "d = defaultdict(list)\n",
    "print('loading', checkpoint)\n",
    "model = prompt_classification.create_model(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating accs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.72s/it]\n"
     ]
    }
   ],
   "source": [
    "print('calculating accs...')\n",
    "for task_name in tqdm(task_names):\n",
    "    for prompt in ['manual']:\n",
    "        for n_shots in [1]: #, 5]: \n",
    "                args.task_name = task_name\n",
    "                args.n_shots = n_shots\n",
    "                (dset, dset_test), check_answer_func, descr = data.get_data(\n",
    "                    args, args.task_name, n_shots=args.n_shots, train_split_frac=args.train_split_frac)\n",
    "                d['checkpoint'].append(checkpoint)\n",
    "                d['prompt'].append(prompt)\n",
    "                d['task_name'].append(task_name)\n",
    "                d['n_shots'].append(n_shots)\n",
    "                if prompt == 'manual':\n",
    "                    prompt_actual = descr\n",
    "                else:\n",
    "                    prompt_actual = prompt\n",
    "                d['prompt_actual'].append(prompt_actual)\n",
    "                batch_size = batch_sizes.get(checkpoint, 16)\n",
    "                if task_name == 'task107_splash_question_to_sql':\n",
    "                    batch_size = max(1, batch_size//4)\n",
    "                loss, acc = prompt_classification.test_model_on_task_with_prefix(\n",
    "                    dset=dset, model=model, prefix=prompt_actual, multi_token=True, verbose=False,\n",
    "                    batch_size=batch_size,\n",
    "                )\n",
    "                d['acc'].append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually inspect prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_name in task_names:\n",
    "    (dset, dset_test), check_answer_func, descr = data.get_data(\n",
    "        args, task_name, n_shots=args.n_shots, train_split_frac=args.train_split_frac)\n",
    "    print(task_name, descr, dset[0], end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "14b67e045ab4e623bbd9f77d231431043e985fd8f169f266aea842e78b0c1086"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
